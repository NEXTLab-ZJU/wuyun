# small version: 1) n_layer = 4
MODEL:
    n_layer: 4
    d_model: 512  # 256
    d_embed: 512
    d_inner: 2048       
    n_head: 8   
    cutoffs: []
    d_head: 64          # d_head = d_embed // n_head
    div_val: 1
    pre_lnorm: True      #apply LayerNorm to the input instead of the output
    mem_len: 512
    clamp_len: -1        #use the same pos embeddings after clamp_len
    same_length: True    #use the same attn length for all tokens
    proj_share_all_but_first: True
    attn_type: 0
    sample_softmax: -1
    adaptive: False
    dropout: 0.1
    dropatt: 0.0         #attention probability dropout rate
    untie_r: False
    init: 'normal'       #parameter initializer to use.
    init_range: 0.1
    proj_init_std: 0.01
    init_std: 0.02       #parameters initialized by N(0, init_std)
    layer_norm_epsilon: 1e-5
    query_dim: 16        #64
    seq_len: 512         #512
    emb_init: 'normal'   #parameter initializer to use.
    emb_init_range: 0.01 #parameters initialized by U(-init_range, init_range)
    ext_len: 0
    tgt_len: 70
    eval_tgt_len: 50
    position_concat: False  
    n_token: 0  

TRAIN: 
    Dataset: 'Wikifonia'
    ROOT: 'paper_tasks/model'
    model_name: 'exp_train_small_4layer_noChord'
    gpuID: '0'
    output_dir: "checkpoint/exp_train_small_4layer_noChord"
    experiment_dir: "checkpoint/exp_train_small_4layer_noChord"
    last_experiment_dir: "checkpoint_last"
    batch_size: 20 
    lr: 0.0001             
    num_epochs: 300
    save_freq: 5
    seed: 1234
    optim: 'adam'    
    no_cuda: False
    resume_training_model: None



INFERENCE:
    num_sample: 100
    gpuID: '5'
    generated_dir: 'paper_tasks/gen_midis'
    checkpoint_type: epoch_idx    # best_train, best_val, epoch_idx
    model_epoch: 90 
    no_cuda: False